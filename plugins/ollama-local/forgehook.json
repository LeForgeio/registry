{
  "$schema": "../../forgehook-schema.json",
  "id": "ollama-local",
  "name": "Ollama",
  "version": "1.1.0",
  "description": "Local AI inference server for running open-source LLMs. Supports Llama 3.2, Mistral, Code Llama, Phi-3, Qwen, and more. Required for LeForge Agent Runtime.",
  "author": {
    "name": "LeForge",
    "url": "https://leforge.io"
  },
  "license": "MIT",
  "repository": "https://github.com/LeForgeio/registry/tree/main/plugins/ollama-local",
  "icon": "bot",
  "category": "ai",
  "tags": ["llm", "ai", "local", "ollama", "inference", "agent-runtime", "required"],
  "featured": true,
  
  "image": {
    "repository": "ollama/ollama",
    "tag": "latest"
  },
  
  "port": 11434,
  "basePath": "/api",
  
  "healthCheck": {
    "path": "/api/tags",
    "interval": 30,
    "timeout": 10,
    "retries": 3
  },
  
  "endpoints": [
    {
      "path": "/generate",
      "method": "POST",
      "description": "Generate text from a prompt using a local model",
      "input": {
        "type": "object",
        "required": ["model", "prompt"],
        "properties": {
          "model": {
            "type": "string",
            "description": "Model name (e.g., llama3.2, mistral, codellama)"
          },
          "prompt": {
            "type": "string",
            "description": "Text prompt to generate from"
          },
          "system": {
            "type": "string",
            "description": "System prompt for context"
          },
          "template": {
            "type": "string",
            "description": "Custom prompt template"
          },
          "context": {
            "type": "array",
            "description": "Context from previous response for multi-turn",
            "items": { "type": "integer" }
          },
          "stream": {
            "type": "boolean",
            "description": "Stream the response",
            "default": false
          },
          "options": {
            "type": "object",
            "description": "Model parameters",
            "properties": {
              "temperature": { "type": "number" },
              "top_p": { "type": "number" },
              "top_k": { "type": "integer" },
              "num_predict": { "type": "integer" }
            }
          }
        }
      },
      "output": {
        "type": "object",
        "properties": {
          "model": { "type": "string" },
          "created_at": { "type": "string" },
          "response": { "type": "string" },
          "done": { "type": "boolean" },
          "context": { "type": "array", "items": { "type": "integer" } },
          "total_duration": { "type": "integer" },
          "eval_count": { "type": "integer" }
        }
      }
    },
    {
      "path": "/chat",
      "method": "POST",
      "description": "Chat with a local model using message format (supports tool calling)",
      "input": {
        "type": "object",
        "required": ["model", "messages"],
        "properties": {
          "model": {
            "type": "string",
            "description": "Model name"
          },
          "messages": {
            "type": "array",
            "description": "Chat messages",
            "items": {
              "type": "object",
              "properties": {
                "role": { "type": "string", "enum": ["system", "user", "assistant", "tool"] },
                "content": { "type": "string" }
              }
            }
          },
          "tools": {
            "type": "array",
            "description": "Available tools/functions for the model to call",
            "items": {
              "type": "object",
              "properties": {
                "type": { "type": "string" },
                "function": {
                  "type": "object",
                  "properties": {
                    "name": { "type": "string" },
                    "description": { "type": "string" },
                    "parameters": { "type": "object" }
                  }
                }
              }
            }
          },
          "stream": {
            "type": "boolean",
            "default": false
          },
          "options": {
            "type": "object",
            "properties": {
              "temperature": { "type": "number" },
              "num_predict": { "type": "integer" }
            }
          }
        }
      },
      "output": {
        "type": "object",
        "properties": {
          "model": { "type": "string" },
          "created_at": { "type": "string" },
          "message": {
            "type": "object",
            "properties": {
              "role": { "type": "string" },
              "content": { "type": "string" },
              "tool_calls": { "type": "array" }
            }
          },
          "done": { "type": "boolean" }
        }
      }
    },
    {
      "path": "/embeddings",
      "method": "POST",
      "description": "Generate embeddings for text",
      "input": {
        "type": "object",
        "required": ["model", "prompt"],
        "properties": {
          "model": {
            "type": "string",
            "description": "Model name (e.g., nomic-embed-text, mxbai-embed-large)"
          },
          "prompt": {
            "type": "string",
            "description": "Text to embed"
          }
        }
      },
      "output": {
        "type": "object",
        "properties": {
          "embedding": {
            "type": "array",
            "items": { "type": "number" }
          }
        }
      }
    },
    {
      "path": "/tags",
      "method": "GET",
      "description": "List locally available models",
      "input": {
        "type": "object",
        "properties": {}
      },
      "output": {
        "type": "object",
        "properties": {
          "models": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "name": { "type": "string" },
                "modified_at": { "type": "string" },
                "size": { "type": "integer" },
                "digest": { "type": "string" }
              }
            }
          }
        }
      }
    },
    {
      "path": "/pull",
      "method": "POST",
      "description": "Download a model from the Ollama library",
      "input": {
        "type": "object",
        "required": ["name"],
        "properties": {
          "name": {
            "type": "string",
            "description": "Model name to pull (e.g., llama3.2, mistral, phi3)"
          },
          "stream": {
            "type": "boolean",
            "default": false
          }
        }
      },
      "output": {
        "type": "object",
        "properties": {
          "status": { "type": "string" }
        }
      }
    },
    {
      "path": "/show",
      "method": "POST",
      "description": "Show model information",
      "input": {
        "type": "object",
        "required": ["name"],
        "properties": {
          "name": { "type": "string" }
        }
      },
      "output": {
        "type": "object",
        "properties": {
          "modelfile": { "type": "string" },
          "parameters": { "type": "string" },
          "template": { "type": "string" }
        }
      }
    },
    {
      "path": "/delete",
      "method": "DELETE",
      "description": "Delete a local model",
      "input": {
        "type": "object",
        "required": ["name"],
        "properties": {
          "name": { "type": "string" }
        }
      }
    }
  ],
  
  "environment": [
    {
      "name": "OLLAMA_HOST",
      "description": "Host address to bind to",
      "default": "0.0.0.0",
      "required": false
    },
    {
      "name": "OLLAMA_MODELS",
      "description": "Path to models directory",
      "default": "/root/.ollama/models",
      "required": false
    },
    {
      "name": "OLLAMA_NUM_PARALLEL",
      "description": "Maximum concurrent requests",
      "default": "1",
      "required": false
    },
    {
      "name": "OLLAMA_MAX_LOADED_MODELS",
      "description": "Maximum models to keep in memory",
      "default": "1",
      "required": false
    }
  ],
  
  "volumes": [
    {
      "name": "ollama-models",
      "containerPath": "/root/.ollama",
      "description": "Persistent storage for downloaded models"
    }
  ],
  
  "resources": {
    "memory": "4g",
    "cpu": "2.0"
  },
  
  "gpu": {
    "enabled": true,
    "optional": true,
    "driver": "nvidia",
    "capabilities": ["compute", "utility"]
  },
  
  "documentation": {
    "overview": "Ollama is a local AI inference server that makes it easy to run open-source LLMs. This plugin is required for the LeForge Agent Runtime to function.",
    "installation": "1. Install this plugin from the marketplace\n2. After installation, pull a model: use the /pull endpoint or run `ollama pull llama3.2` in the container\n3. The Agent Runtime will automatically detect available models",
    "models": "Recommended models:\n- llama3.2 (3B) - Fast, good for general tasks\n- llama3.1 (8B) - Better quality, more memory\n- mistral (7B) - Excellent for code and reasoning\n- phi3 (3.8B) - Microsoft's efficient model\n- codellama (7B) - Specialized for code",
    "gpu": "GPU acceleration is automatically enabled if an NVIDIA GPU is detected. For CPU-only systems, models will run slower but still work.",
    "examples": [
      {
        "title": "Pull a Model",
        "description": "Download llama3.2 for use with agents",
        "request": {
          "method": "POST",
          "path": "/api/pull",
          "body": {
            "name": "llama3.2",
            "stream": false
          }
        }
      },
      {
        "title": "Chat with Tool Calling",
        "description": "Chat with function/tool support",
        "request": {
          "method": "POST",
          "path": "/api/chat",
          "body": {
            "model": "llama3.2",
            "messages": [
              { "role": "user", "content": "What files are in my documents folder?" }
            ],
            "tools": [
              {
                "type": "function",
                "function": {
                  "name": "list_files",
                  "description": "List files in a directory",
                  "parameters": {
                    "type": "object",
                    "properties": {
                      "path": { "type": "string" }
                    }
                  }
                }
              }
            ]
          }
        }
      }
    ]
  },
  
  "links": {
    "documentation": "https://github.com/ollama/ollama/blob/main/docs/api.md",
    "models": "https://ollama.com/library",
    "support": "https://github.com/ollama/ollama/issues"
  }
}
