# =============================================================================
# LLM Service Environment Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# Service Configuration
# -----------------------------------------------------------------------------
ENVIRONMENT=development
SERVICE_NAME=llm-service
SERVICE_VERSION=2.0.0

# Server settings
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=DEBUG
WORKERS=1

# CORS (comma-separated origins or * for all)
CORS_ORIGINS=*

# -----------------------------------------------------------------------------
# Default Provider
# -----------------------------------------------------------------------------
# Options: vllm, openai, anthropic, azure, google, bedrock, huggingface
DEFAULT_PROVIDER=vllm

# -----------------------------------------------------------------------------
# vLLM Configuration (Local/Self-hosted)
# -----------------------------------------------------------------------------
# URL of the vLLM server (OpenAI-compatible API)
VLLM_BASE_URL=http://localhost:8001/v1

# Default model to use for inference
# Popular options:
# - TinyLlama/TinyLlama-1.1B-Chat-v1.0 (small, fast)
# - mistralai/Mistral-7B-Instruct-v0.2 (good balance)
# - meta-llama/Llama-2-13b-chat-hf (larger, better quality)
DEFAULT_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0

# -----------------------------------------------------------------------------
# OpenAI Configuration
# -----------------------------------------------------------------------------
# OPENAI_API_KEY=sk-...
# OPENAI_ORGANIZATION=org-...
OPENAI_DEFAULT_MODEL=gpt-4o-mini

# -----------------------------------------------------------------------------
# Anthropic Configuration
# -----------------------------------------------------------------------------
# ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_DEFAULT_MODEL=claude-3-5-sonnet-20241022

# -----------------------------------------------------------------------------
# Azure OpenAI Configuration
# -----------------------------------------------------------------------------
# AZURE_OPENAI_API_KEY=...
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_DEPLOYMENT=gpt-4
AZURE_OPENAI_API_VERSION=2024-02-01

# -----------------------------------------------------------------------------
# Google AI / Vertex AI Configuration
# -----------------------------------------------------------------------------
# GOOGLE_API_KEY=...
# GOOGLE_PROJECT=your-project-id
GOOGLE_REGION=us-central1
GOOGLE_DEFAULT_MODEL=gemini-1.5-flash

# -----------------------------------------------------------------------------
# AWS Bedrock Configuration
# -----------------------------------------------------------------------------
# AWS_ACCESS_KEY_ID=...
# AWS_SECRET_ACCESS_KEY=...
AWS_REGION=us-east-1
BEDROCK_DEFAULT_MODEL=anthropic.claude-3-5-sonnet-20241022-v2:0

# -----------------------------------------------------------------------------
# Hugging Face Configuration
# -----------------------------------------------------------------------------
# HUGGINGFACE_API_KEY=hf_...
# HUGGINGFACE_ENDPOINT=https://your-endpoint.endpoints.huggingface.cloud
HUGGINGFACE_DEFAULT_MODEL=meta-llama/Llama-3.2-3B-Instruct

# -----------------------------------------------------------------------------
# Generation defaults
# -----------------------------------------------------------------------------
DEFAULT_MAX_TOKENS=512
DEFAULT_TEMPERATURE=0.7
DEFAULT_TOP_P=1.0

# Token limits
MAX_INPUT_TOKENS=4096
MAX_OUTPUT_TOKENS=2048

# Request timeout in seconds
REQUEST_TIMEOUT=120

# -----------------------------------------------------------------------------
# Redis Configuration (for request queuing)
# -----------------------------------------------------------------------------
REDIS_URL=redis://localhost:6379/0
ENABLE_QUEUE=true

# Queue settings
QUEUE_NAME=llm_requests
MAX_QUEUE_SIZE=1000
JOB_TIMEOUT=300

# -----------------------------------------------------------------------------
# Embeddings Configuration
# -----------------------------------------------------------------------------
ENABLE_EMBEDDINGS=true

# Embedding model (sentence-transformers)
# Options:
# - all-MiniLM-L6-v2 (fast, 384 dims)
# - all-mpnet-base-v2 (better quality, 768 dims)
# - paraphrase-multilingual-MiniLM-L12-v2 (multilingual)
DEFAULT_EMBEDDING_MODEL=all-MiniLM-L6-v2

# Batch size for embedding generation
EMBEDDING_BATCH_SIZE=32

# -----------------------------------------------------------------------------
# Vision Configuration
# -----------------------------------------------------------------------------
ENABLE_VISION=true
VISION_MODEL=llava-hf/llava-1.5-7b-hf
MAX_IMAGE_SIZE_MB=10

# -----------------------------------------------------------------------------
# Authentication (optional)
# -----------------------------------------------------------------------------
# API_KEY=your-api-key-here
# REQUIRE_AUTH=false

# -----------------------------------------------------------------------------
# Monitoring (optional)
# -----------------------------------------------------------------------------
ENABLE_METRICS=false
# METRICS_PORT=9090

# -----------------------------------------------------------------------------
# Model Cache
# -----------------------------------------------------------------------------
MODEL_CACHE_DIR=/tmp/llm-service/models
# HF_HOME=/path/to/cache
